"""
Analytics Insights Generation Engine
Generates comprehensive insights from multi-platform analytics data
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
from models import AnalyticsInsights

logger = logging.getLogger(__name__)


@dataclass
class AnalyticsData:
    """Container for user analytics data"""
    total_publications: int
    platforms_data: Dict[str, List[Dict]]
    time_period: str
    user_id: str
    
    def has_sufficient_data(self) -> bool:
        """Check if there's enough data for insights generation"""
        return self.total_publications >= 3  # Minimum 3 publications for insights


class AnalyticsInsightsGenerator:
    """
    Analytics Insights Generation Engine
    Implements AI-powered insights generation from multi-platform data
    """
    
    def __init__(self, chromadb_manager):
        self.chromadb_manager = chromadb_manager
        
        # Platform-specific insight patterns
        self.platform_patterns = {
            "linkedin": {
                "engagement_metrics": ["views", "reactions", "comments", "shares"],
                "primary_metric": "reactions",
                "engagement_benchmark": 0.05,  # 5% engagement rate is good
                "optimal_times": ["Tuesday 9AM", "Wednesday 2PM", "Thursday 10AM"]
            },
            "twitter": {
                "engagement_metrics": ["views", "likes", "retweets", "replies"],
                "primary_metric": "likes",
                "engagement_benchmark": 0.02,  # 2% engagement rate is good
                "optimal_times": ["Wednesday 2PM", "Thursday 1PM", "Friday 11AM"]
            },
            "ghost": {
                "engagement_metrics": ["views", "likes", "comments"],
                "primary_metric": "views",
                "engagement_benchmark": 0.10,  # 10% return rate is good
                "optimal_times": ["Thursday 8AM", "Tuesday 9AM", "Wednesday 7AM"]
            },
            "beehiiv": {
                "engagement_metrics": ["opens", "clicks", "subscribers"],
                "primary_metric": "opens",
                "engagement_benchmark": 0.25,  # 25% open rate is good
                "optimal_times": ["Tuesday 8AM", "Thursday 9AM", "Sunday 10AM"]
            }
        }
        
        # Content type patterns
        self.content_patterns = {
            "thought_leadership": {
                "platforms": ["linkedin"],
                "avg_engagement_boost": 1.2,
                "optimal_length": "800-1200 words"
            },
            "tutorial": {
                "platforms": ["ghost", "linkedin"],
                "avg_engagement_boost": 1.4,
                "optimal_length": "1500-2500 words"
            },
            "industry_update": {
                "platforms": ["twitter", "linkedin"],
                "avg_engagement_boost": 0.9,
                "optimal_length": "300-600 words"
            },
            "newsletter": {
                "platforms": ["beehiiv"],
                "avg_engagement_boost": 1.1,
                "optimal_length": "800-1500 words"
            }
        }
    
    async def generate_comprehensive_insights(self, user_id: str, time_period: str = "30d",
                                            platforms: Optional[List[str]] = None,
                                            content_types: Optional[List[str]] = None) -> AnalyticsInsights:
        """Generate comprehensive analytics insights"""
        try:
            logger.info(f"ðŸ§  Generating insights for user {user_id}, period: {time_period}")
            
            # Query user analytics data
            analytics_data = await self._prepare_analytics_data(user_id, time_period, platforms)
            
            if not analytics_data.has_sufficient_data():
                raise ValueError(f"Insufficient data for insights generation. Found {analytics_data.total_publications} publications, minimum 3 required.")
            
            # Generate performance analysis
            overall_performance = await self._analyze_overall_performance(analytics_data)
            platform_comparison = await self._analyze_platform_performance(analytics_data)
            content_type_performance = await self._analyze_content_performance(analytics_data)
            trending_topics = await self._detect_trending_topics(analytics_data)
            
            # Generate insights and recommendations
            key_insights = await self._generate_key_insights(analytics_data, platform_comparison, content_type_performance)
            recommendations = await self._generate_recommendations(analytics_data, platform_comparison, content_type_performance)
            optimal_schedule = await self._generate_optimal_schedule(analytics_data, platform_comparison)
            
            # Calculate data quality metrics
            data_coverage = await self._calculate_data_coverage(analytics_data)
            data_quality_score = await self._calculate_data_quality_score(analytics_data)
            
            # Create comprehensive insights response
            insights = AnalyticsInsights(
                user_id=user_id,
                time_period=time_period,
                total_publications=analytics_data.total_publications,
                platforms_analyzed=list(analytics_data.platforms_data.keys()),
                overall_performance=overall_performance,
                platform_comparison=platform_comparison,
                content_type_performance=content_type_performance,
                trending_topics=trending_topics,
                key_insights=key_insights,
                recommendations=recommendations,
                optimal_posting_schedule=optimal_schedule,
                data_coverage=data_coverage,
                data_quality_score=data_quality_score,
                generated_at=datetime.now()
            )
            
            # Store insights for future reference
            await self.chromadb_manager.store_generated_insights(user_id, insights.dict())
            
            logger.info(f"âœ… Generated insights for user {user_id}")
            return insights
            
        except Exception as e:
            logger.error(f"âŒ Insights generation failed: {e}")
            raise
    
    async def _prepare_analytics_data(self, user_id: str, time_period: str, 
                                     platforms: Optional[List[str]]) -> AnalyticsData:
        """Prepare and structure analytics data for analysis"""
        try:
            # Query raw analytics data
            raw_data = await self.chromadb_manager.query_user_analytics(
                user_id=user_id,
                platforms=platforms,
                time_period=time_period
            )
            
            # Structure data by platform
            platforms_data = {}
            total_publications = 0
            
            for record in raw_data:
                metadata = record.get("metadata", {})
                platform = metadata.get("platform")
                
                if platform:
                    if platform not in platforms_data:
                        platforms_data[platform] = []
                    
                    platforms_data[platform].append({
                        "publication_id": metadata.get("publication_id"),
                        "metrics": metadata.get("metrics", {}),
                        "quality_score": metadata.get("quality_score", 0.0),
                        "collected_at": metadata.get("collected_at"),
                        "collection_method": metadata.get("collection_method")
                    })
                    
                    total_publications += 1
            
            return AnalyticsData(
                total_publications=total_publications,
                platforms_data=platforms_data,
                time_period=time_period,
                user_id=user_id
            )
            
        except Exception as e:
            logger.error(f"Analytics data preparation failed: {e}")
            raise
    
    async def _analyze_overall_performance(self, analytics_data: AnalyticsData) -> Dict[str, float]:
        """Analyze overall performance across all platforms"""
        total_reach = 0
        total_engagement = 0
        total_publications = analytics_data.total_publications
        
        for platform, records in analytics_data.platforms_data.items():
            platform_config = self.platform_patterns.get(platform, {})
            engagement_metrics = platform_config.get("engagement_metrics", [])
            
            for record in records:
                metrics = record.get("metrics", {})
                
                # Calculate reach (views/impressions)
                reach_metrics = ["views", "impressions", "opens"]
                for metric in reach_metrics:
                    if metric in metrics:
                        total_reach += metrics[metric]
                        break
                
                # Calculate engagement
                engagement_count = 0
                for metric in engagement_metrics:
                    if metric in metrics:
                        engagement_count += metrics[metric]
                
                total_engagement += engagement_count
        
        # Calculate performance metrics
        avg_reach = total_reach / total_publications if total_publications > 0 else 0
        avg_engagement = total_engagement / total_publications if total_publications > 0 else 0
        engagement_rate = total_engagement / total_reach if total_reach > 0 else 0
        
        return {
            "avg_engagement_rate": round(engagement_rate, 3),
            "total_reach": total_reach,
            "total_interactions": total_engagement,
            "avg_reach_per_post": round(avg_reach, 1),
            "avg_engagement_per_post": round(avg_engagement, 1)
        }
    
    async def _analyze_platform_performance(self, analytics_data: AnalyticsData) -> Dict[str, Dict[str, float]]:
        """Analyze performance by platform"""
        platform_comparison = {}
        
        for platform, records in analytics_data.platforms_data.items():
            if not records:
                continue
            
            platform_config = self.platform_patterns.get(platform, {})
            primary_metric = platform_config.get("primary_metric", "views")
            
            # Calculate platform-specific metrics
            total_reach = 0
            total_engagement = 0
            publication_count = len(records)
            
            for record in records:
                metrics = record.get("metrics", {})
                
                # Get reach metric
                reach_metrics = ["views", "impressions", "opens"]
                for metric in reach_metrics:
                    if metric in metrics:
                        total_reach += metrics[metric]
                        break
                
                # Get primary engagement metric
                if primary_metric in metrics:
                    total_engagement += metrics[primary_metric]
            
            avg_reach = total_reach / publication_count if publication_count > 0 else 0
            avg_engagement = total_engagement / publication_count if publication_count > 0 else 0
            engagement_rate = total_engagement / total_reach if total_reach > 0 else 0
            
            platform_comparison[platform] = {
                "avg_views": round(avg_reach, 1),
                "avg_engagement_rate": round(engagement_rate, 3),
                "total_publications": publication_count,
                "best_performing_type": "general"  # Simplified for now
            }
        
        return platform_comparison
    
    async def _analyze_content_performance(self, analytics_data: AnalyticsData) -> Dict[str, Dict[str, float]]:
        """Analyze performance by content type"""
        # Simplified content type analysis
        content_performance = {}
        
        # Default content types for demonstration
        content_types = ["thought_leadership", "tutorial", "industry_update"]
        
        for content_type in content_types:
            # Simulate content type analysis
            # In production, this would analyze actual content classification
            
            estimated_posts = analytics_data.total_publications // len(content_types)
            avg_engagement = 0.08 + (hash(content_type) % 5) * 0.01  # Simulated variance
            
            # Determine best platform for content type
            pattern = self.content_patterns.get(content_type, {})
            best_platforms = pattern.get("platforms", list(analytics_data.platforms_data.keys()))
            best_platform = best_platforms[0] if best_platforms else "linkedin"
            
            content_performance[content_type] = {
                "total_posts": estimated_posts,
                "avg_engagement": round(avg_engagement, 3),
                "best_platform": best_platform
            }
        
        return content_performance
    
    async def _detect_trending_topics(self, analytics_data: AnalyticsData) -> List[Dict[str, Any]]:
        """Detect trending topics from analytics data"""
        # Simplified trending topics detection
        # In production, this would use NLP to analyze content
        
        trending_topics = [
            {
                "topic": "AI in software development",
                "mentions": 3,
                "avg_performance": 0.145,
                "trend_score": 0.89
            },
            {
                "topic": "Remote work productivity",
                "mentions": 2,
                "avg_performance": 0.112,
                "trend_score": 0.76
            }
        ]
        
        return trending_topics
    
    async def _generate_key_insights(self, analytics_data: AnalyticsData, 
                                   platform_comparison: Dict, content_performance: Dict) -> List[str]:
        """Generate key insights from analytics data"""
        insights = []
        
        # Platform performance insights
        if len(platform_comparison) > 1:
            # Find best performing platform
            best_platform = max(platform_comparison.keys(), 
                              key=lambda p: platform_comparison[p]["avg_engagement_rate"])
            best_rate = platform_comparison[best_platform]["avg_engagement_rate"]
            
            # Compare with other platforms
            other_platforms = [p for p in platform_comparison.keys() if p != best_platform]
            if other_platforms:
                avg_other_rate = sum(platform_comparison[p]["avg_engagement_rate"] 
                                   for p in other_platforms) / len(other_platforms)
                
                if best_rate > avg_other_rate * 1.2:
                    improvement = ((best_rate - avg_other_rate) / avg_other_rate) * 100
                    insights.append(f"{best_platform.title()} content performs {improvement:.0f}% better than other platforms")
        
        # Engagement patterns
        overall_engagement = sum(p["avg_engagement_rate"] for p in platform_comparison.values()) / len(platform_comparison)
        if overall_engagement > 0.08:
            insights.append("Your content consistently achieves above-average engagement rates")
        elif overall_engagement > 0.05:
            insights.append("Your engagement rates are solid with room for optimization")
        else:
            insights.append("Focus on engagement optimization - current rates are below benchmarks")
        
        # Content type insights
        best_content = max(content_performance.keys(), 
                          key=lambda c: content_performance[c]["avg_engagement"])
        insights.append(f"{best_content.replace('_', ' ').title()} content drives your highest engagement")
        
        # Timing insights (simplified)
        insights.append("Tuesday morning posts consistently get higher engagement than weekend posts")
        
        return insights
    
    async def _generate_recommendations(self, analytics_data: AnalyticsData,
                                      platform_comparison: Dict, content_performance: Dict) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Platform-specific recommendations
        best_platform = max(platform_comparison.keys(), 
                           key=lambda p: platform_comparison[p]["avg_engagement_rate"])
        
        recommendations.append(f"Focus more content on {best_platform.title()} - your best performing platform")
        
        # Content optimization recommendations
        best_content_type = max(content_performance.keys(), 
                               key=lambda c: content_performance[c]["avg_engagement"])
        recommendations.append(f"Expand {best_content_type.replace('_', ' ')} content - it drives highest engagement")
        
        # Timing recommendations
        for platform in platform_comparison.keys():
            if platform in self.platform_patterns:
                optimal_times = self.platform_patterns[platform].get("optimal_times", [])
                if optimal_times:
                    recommendations.append(f"Schedule {platform.title()} posts for {optimal_times[0]}")
        
        # Cross-platform recommendations
        if len(platform_comparison) > 1:
            recommendations.append("Cross-promote high-performing content across platforms within 2 hours")
        
        return recommendations
    
    async def _generate_optimal_schedule(self, analytics_data: AnalyticsData,
                                       platform_comparison: Dict) -> Dict[str, str]:
        """Generate optimal posting schedule by platform"""
        optimal_schedule = {}
        
        for platform in platform_comparison.keys():
            if platform in self.platform_patterns:
                optimal_times = self.platform_patterns[platform].get("optimal_times", [])
                if optimal_times:
                    optimal_schedule[platform] = optimal_times[0]
                else:
                    optimal_schedule[platform] = "Tuesday 10:00 AM EST"
            else:
                optimal_schedule[platform] = "Tuesday 10:00 AM EST"
        
        return optimal_schedule
    
    async def _calculate_data_coverage(self, analytics_data: AnalyticsData) -> Dict[str, float]:
        """Calculate data coverage by platform"""
        data_coverage = {}
        
        for platform, records in analytics_data.platforms_data.items():
            # Calculate coverage based on data completeness
            if not records:
                data_coverage[platform] = 0.0
                continue
            
            total_quality = sum(record.get("quality_score", 0.0) for record in records)
            avg_quality = total_quality / len(records)
            
            # Adjust based on collection method
            method_adjustment = {
                "api": 1.0,      # Full API access
                "proxy": 0.8,    # Limited proxy access
                "manual": 0.6,   # Manual entry limitations
                "csv": 0.9       # Complete but delayed data
            }
            
            # Get collection method from first record
            collection_method = records[0].get("collection_method", "manual")
            adjustment = method_adjustment.get(collection_method, 0.7)
            
            coverage = avg_quality * adjustment
            data_coverage[platform] = round(coverage, 2)
        
        return data_coverage
    
    async def _calculate_data_quality_score(self, analytics_data: AnalyticsData) -> float:
        """Calculate overall data quality score"""
        total_quality = 0.0
        total_records = 0
        
        for platform, records in analytics_data.platforms_data.items():
            for record in records:
                total_quality += record.get("quality_score", 0.0)
                total_records += 1
        
        if total_records == 0:
            return 0.0
        
        overall_quality = total_quality / total_records
        return round(overall_quality, 2)